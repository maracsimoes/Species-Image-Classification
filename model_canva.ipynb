{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Any, Self\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Keras imports (Core)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Input, Lambda, Flatten, Dropout\n",
    "\n",
    "from keras.layers import RandomTranslation, RandomRotation, RandomZoom, RandomFlip, RandomContrast\n",
    "\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.metrics import CategoricalAccuracy, AUC, F1Score \n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler\n",
    "from keras.utils import image_dataset_from_directory\n",
    "\n",
    "# Keras Applications imports\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collab Setting\n",
    "If you are using Google Colab, run the following cell to set up google drive mount. \n",
    "\n",
    "Also remember to change paths when needed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "## If using Google Colab, set the data_dir_path to the mounted drive\n",
    "# root_dir_path = Path('/content/drive/MyDrive')\n",
    "\n",
    "root_dir_path = Path(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ds(\n",
    "    dir_path: Path,\n",
    "    batch_size: int,\n",
    "    input_shape: tuple[int, ...],\n",
    "    shuffle: bool = True\n",
    ") -> Any:\n",
    "\n",
    "    height, width, n_channels = input_shape\n",
    "    image_size = (height, width)\n",
    "\n",
    "    ds = image_dataset_from_directory(\n",
    "        dir_path,\n",
    "        label_mode=\"categorical\",\n",
    "        batch_size=batch_size,\n",
    "        image_size=image_size,\n",
    "        interpolation=\"bilinear\",\n",
    "        shuffle=shuffle,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_decay_lr_scheduler(\n",
    "    epoch: int,\n",
    "    current_lr: float,\n",
    "    factor: float = 0.95\n",
    ") -> float:\n",
    "\n",
    "    current_lr *= factor\n",
    "\n",
    "    return current_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape=(299, 299, 3), n_classes=202, model_name=\"UnnamedModel\") -> Self:\n",
    "   \n",
    "    # Define the input tensor\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Augmentation layer\n",
    "\n",
    "    x = RandomRotation(0.2)(x)\n",
    "    x = RandomTranslation(0.1, 0.1)(x)\n",
    "    x = RandomZoom(0.1)(x)\n",
    "    x = RandomFlip(\"horizontal\")(x)\n",
    "    x = RandomContrast(0.2)(x)\n",
    "\n",
    "    \n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(n_classes, activation='softmax')(x)\n",
    "\n",
    "    # Create the complete model\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"UnnamedModel\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Train Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Model Independent Parameters\n",
    "n_classes = 202\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "input_shape = (299, 299, 3)\n",
    "image_size = (299, 299)\n",
    "value_range = (-1.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr = 0.001\n",
    "final_lr = 0.0003\n",
    "factor = (final_lr / initial_lr) ** (1 / epochs)\n",
    "lr_scheduler = partial(exp_decay_lr_scheduler, factor=factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_accuracy = CategoricalAccuracy(name=\"accuracy\")\n",
    "precision = CategoricalAccuracy(name=\"precision\")\n",
    "recall = CategoricalAccuracy(name=\"recall\")\n",
    "f1_score = F1Score(average=\"macro\", name=\"f1_score\")\n",
    "metrics = [categorical_accuracy,precision, recall, f1_score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_path = root_dir_path / \"data\"\n",
    "\n",
    "train_dir_path = data_dir_path / \"train\"\n",
    "val_dir_path = data_dir_path / \"val\"\n",
    "test_dir_path = data_dir_path / \"test\"\n",
    "\n",
    "_load_ds = partial(\n",
    "        load_ds, batch_size=batch_size, input_shape=input_shape\n",
    "    )\n",
    "\n",
    "train_ds = _load_ds(train_dir_path)\n",
    "val_ds = _load_ds(val_dir_path, shuffle=False)\n",
    "test_ds = _load_ds(test_dir_path, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "optimizer = Adam(learning_rate=initial_lr)\n",
    "loss = CategoricalCrossentropy() \n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Define callbacks\n",
    "metrics_file_path = root_dir_path / f\"{timestamp}_{model.name}_metrics.csv\"\n",
    "checkpoint_file_path = root_dir_path / f\"{timestamp}_{model.name}_checkpoint.keras\"\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    checkpoint_file_path, monitor=\"val_loss\", verbose=0\n",
    ")\n",
    "metrics_callback = CSVLogger(metrics_file_path)\n",
    "lr_scheduler_callback = LearningRateScheduler(lr_scheduler)\n",
    "callbacks = [checkpoint_callback, metrics_callback, lr_scheduler_callback]\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "_ = model.fit(\n",
    "    train_ds, validation_data=val_ds, epochs=epochs, callbacks=callbacks, verbose=1\n",
    ")\n",
    "\n",
    "evaluation_dict = model.evaluate(test_ds, return_dict=True, verbose=0)\n",
    "\n",
    "print(evaluation_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ploting the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.read_csv(metrics_file_path)\n",
    "\n",
    "df_metrics.head()\n",
    "\n",
    "# Load the metrics CSV file\n",
    "metrics_df = pd.read_csv(metrics_file_path)\n",
    "\n",
    "# Create a figure with 2 rows and 2 columns\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Plot Loss\n",
    "axs[0, 0].plot(metrics_df['epoch'], metrics_df['loss'], 'b-', label='Training')\n",
    "axs[0, 0].plot(metrics_df['epoch'], metrics_df['val_loss'], 'r-', label='Validation')\n",
    "axs[0, 0].set_title('Loss')\n",
    "axs[0, 0].set_xlabel('Epoch')\n",
    "axs[0, 0].set_ylabel('Loss')\n",
    "axs[0, 0].legend()\n",
    "\n",
    "# Plot Accuracy\n",
    "axs[0, 1].plot(metrics_df['epoch'], metrics_df['accuracy'], 'b-', label='Training')\n",
    "axs[0, 1].plot(metrics_df['epoch'], metrics_df['val_accuracy'], 'r-', label='Validation')\n",
    "axs[0, 1].set_title('Accuracy')\n",
    "axs[0, 1].set_xlabel('Epoch')\n",
    "axs[0, 1].set_ylabel('Accuracy')\n",
    "axs[0, 1].legend()\n",
    "\n",
    "\n",
    "# Plot F1 Score\n",
    "axs[1, 0].plot(metrics_df['epoch'], metrics_df['f1_score'], 'b-', label='Training')\n",
    "axs[1, 0].plot(metrics_df['epoch'], metrics_df['val_f1_score'], 'r-', label='Validation')\n",
    "axs[1, 0].set_title('F1 Score')\n",
    "axs[1, 0].set_xlabel('Epoch')\n",
    "axs[1, 0].set_ylabel('F1 Score')\n",
    "axs[1, 0].legend()\n",
    "\n",
    "# Plot Precision\n",
    "axs[1, 1].plot(metrics_df['epoch'], metrics_df['precision'], 'b-', label='Training')\n",
    "axs[1, 1].plot(metrics_df['epoch'], metrics_df['val_precision'], 'r-', label='Validation')\n",
    "axs[1, 1].set_title('Precision')\n",
    "axs[1, 1].set_xlabel('Epoch')\n",
    "axs[1, 1].set_ylabel('Precision')\n",
    "axs[1, 1].legend()\n",
    "\n",
    "# Plot Precision\n",
    "axs[2, 0].plot(metrics_df['epoch'], metrics_df['recall'], 'b-', label='Training')\n",
    "axs[2, 0].plot(metrics_df['epoch'], metrics_df['val_recall'], 'r-', label='Validation')\n",
    "axs[2, 0].set_title('Recall')\n",
    "axs[2, 0].set_xlabel('Epoch')\n",
    "axs[2, 0].set_ylabel('Recall')\n",
    "axs[2, 0].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefigroot_dir_path / \"models\" / f\"{timestamp}_{model.name}_training_metrics.png\", dpi=300\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models\\20250401_220339_AugmentedXception.keras\n"
     ]
    }
   ],
   "source": [
    "# Create a timestamped model name\n",
    "model_save_path = root_dir_path / 'models' / f\"{timestamp}_{model.name}.keras\"\n",
    "\n",
    "# Save the entire model\n",
    "model.save(model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\n",
      "auc\n",
      "f1_score\n"
     ]
    }
   ],
   "source": [
    "for n in metrics:\n",
    "    print(n.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bundle your config into a dict\n",
    "config = {\n",
    "    \"n_classes\": n_classes,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": epochs,\n",
    "    \"input_shape\": input_shape,\n",
    "    \"image_size\": image_size,\n",
    "    \"value_range\": value_range,\n",
    "    \"initial_lr\": initial_lr,\n",
    "    \"final_lr\": final_lr,\n",
    "    #\"lr_scheduler\": lr_scheduler,\n",
    "    \"metrics\": [metric.name for metric in metrics],\n",
    "    \"optimizer\": optimizer.name,\n",
    "    \"loss\": loss.name,\n",
    "}\n",
    "\n",
    "config_save_path = root_dir_path / 'models' / f\"{timestamp}_{model.name}_config.json\"\n",
    "# Save to a pickle file\n",
    "with open(config_save_path, \"w\") as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_classes': 202,\n",
       " 'batch_size': 64,\n",
       " 'epochs': 10,\n",
       " 'input_shape': [299, 299, 3],\n",
       " 'image_size': [299, 299],\n",
       " 'value_range': [-1.0, 1.0],\n",
       " 'initial_lr': 0.001,\n",
       " 'final_lr': 0.0003,\n",
       " 'factor': 0.8865681505652133,\n",
       " 'metrics': ['accuracy', 'auc', 'f1_score'],\n",
       " 'optimizer': 'adam',\n",
       " 'loss': 'categorical_crossentropy'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(config_save_path, \"r\") as f:\n",
    "    loaded_config = json.load(f)\n",
    "\n",
    "\n",
    "loaded_config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
